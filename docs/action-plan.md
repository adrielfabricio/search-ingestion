# Plano de A√ß√£o - Implementa√ß√£o do Sistema de Ingest√£o e Busca Sem√¢ntica

## Vis√£o Geral

Este documento apresenta um plano de a√ß√£o passo a passo para implementar o sistema completo de ingest√£o e busca sem√¢ntica baseado em RAG (Retrieval Augmented Generation) usando LangChain, PostgreSQL com pgVector.

## üìã Pr√©-requisitos

Antes de come√ßar, verifique:

- [ ] Python 3.9+ instalado
- [ ] Docker e Docker Compose instalados
- [ ] Git instalado (opcional)
- [ ] Conta OpenAI ou Google Gemini com API Key
- [ ] Arquivo PDF para ingest√£o (`document.pdf`)

## üéØ Fase 1: Configura√ß√£o do Ambiente

### Passo 1.1: Preparar Estrutura do Projeto

```bash
# 1. Navegar para o diret√≥rio do projeto
cd search-ingestion

# 2. Criar ambiente virtual Python
python3 -m venv venv

# 3. Ativar ambiente virtual
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 4. Verificar Python e pip
python --version
pip --version
```

**Checkpoint**: Ambiente virtual criado e ativado.

### Passo 1.2: Instalar Depend√™ncias

```bash
# Instalar todas as depend√™ncias do requirements.txt
pip install -r requirements.txt

# Verificar instala√ß√£o das principais bibliotecas
python -c "import langchain; print('LangChain OK')"
python -c "import langchain_openai; print('LangChain OpenAI OK')"
python -c "import langchain_postgres; print('LangChain Postgres OK')"
```

**Checkpoint**: Todas as depend√™ncias instaladas sem erros.

### Passo 1.3: Configurar Vari√°veis de Ambiente

```bash
# 1. Criar arquivo .env a partir do template
cp .env.example .env

# 2. Editar .env com suas credenciais
# Usar editor de sua prefer√™ncia (nano, vim, code, etc.)
nano .env
```

**Arquivo `.env` deve conter:**

```env
# OpenAI (para embeddings e LLM)
OPENAI_API_KEY=sk-...

# OU Google Gemini (alternativa)
# GOOGLE_API_KEY=...

# Caminho do PDF
PDF_PATH=document.pdf

# String de conex√£o do PostgreSQL
POSTGRES_CONNECTION_STRING=postgresql://postgres:postgres@localhost:5432/rag
```

**Checkpoint**: Arquivo `.env` criado e configurado corretamente.

### Passo 1.4: Verificar PDF Dispon√≠vel

```bash
# Verificar se o PDF existe
ls -lh document.pdf

# Ou verificar caminho configurado no .env
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print(os.getenv('PDF_PATH'))"
```

**Checkpoint**: PDF existe e caminho est√° correto.

## üóÑÔ∏è Fase 2: Configura√ß√£o do Banco de Dados

### Passo 2.1: Iniciar PostgreSQL com Docker

```bash
# 1. Subir servi√ßos do Docker Compose
docker compose up -d

# 2. Verificar se os containers est√£o rodando
docker compose ps

# 3. Ver logs para confirmar inicializa√ß√£o
docker compose logs postgres

# Aguardar mensagem: "database system is ready to accept connections"
```

**Checkpoint**: PostgreSQL est√° rodando e saud√°vel.

### Passo 2.2: Verificar Extens√£o pgVector

```bash
# 1. Conectar ao banco de dados
docker compose exec postgres psql -U postgres -d rag

# 2. Dentro do psql, verificar extens√£o
\dx

# Deve mostrar: vector | 0.7.0 | public | pgvector

# 3. Sair do psql
\q
```

**Checkpoint**: Extens√£o pgVector criada com sucesso.

### Passo 2.3: Testar Conex√£o Python

```python
# Criar script tempor√°rio de teste
# test_connection.py
import os
from dotenv import load_dotenv
import psycopg

load_dotenv()

connection_string = os.getenv("POSTGRES_CONNECTION_STRING")

try:
    conn = psycopg.connect(connection_string)
    print("‚úÖ Conex√£o com PostgreSQL estabelecida!")
    conn.close()
except Exception as e:
    print(f"‚ùå Erro na conex√£o: {e}")
```

```bash
python test_connection.py
# Deve imprimir: ‚úÖ Conex√£o com PostgreSQL estabelecida!
```

**Checkpoint**: Conex√£o Python ‚Üí PostgreSQL funcionando.

## üì• Fase 3: Implementa√ß√£o do M√≥dulo de Ingest√£o

### Passo 3.1: Implementar Carregamento do PDF

**Arquivo**: `src/ingest.py`

```python
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader

load_dotenv()

PDF_PATH = os.getenv("PDF_PATH", "document.pdf")

def load_pdf(file_path: str):
    """Carrega arquivo PDF e extrai texto."""
    print(f"üìÑ Carregando PDF: {file_path}...")
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        print(f"‚úÖ PDF carregado! {len(documents)} p√°ginas encontradas.")
        return documents
    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo {file_path} n√£o encontrado!")
        raise
    except Exception as e:
        print(f"‚ùå Erro ao carregar PDF: {e}")
        raise

# Teste
if __name__ == "__main__":
    docs = load_pdf(PDF_PATH)
    print(f"Primeira p√°gina: {docs[0].page_content[:100]}...")
```

**A√ß√£o**: Implementar e testar.

```bash
python src/ingest.py
```

**Checkpoint**: PDF √© carregado corretamente.

### Passo 3.2: Implementar Divis√£o em Chunks

**Arquivo**: `src/ingest.py` (continuar)

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

def split_documents(documents):
    """Divide documentos em chunks de 1000 caracteres com overlap de 150."""
    print("‚úÇÔ∏è  Dividindo documentos em chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150
    )
    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ {len(chunks)} chunks criados!")
    return chunks
```

**A√ß√£o**: Adicionar fun√ß√£o e testar.

```bash
python src/ingest.py
# Deve mostrar n√∫mero de chunks criados
```

**Checkpoint**: Chunks s√£o criados corretamente (1000 chars, 150 overlap).

### Passo 3.3: Configurar Embeddings

**Arquivo**: `src/ingest.py` (continuar)

```python
from langchain_openai import OpenAIEmbeddings
# OU
# from langchain_google_genai import GoogleGenerativeAIEmbeddings

def get_embeddings():
    """Retorna inst√¢ncia de embeddings configurada."""
    # Verificar qual API key est√° dispon√≠vel
    if os.getenv("OPENAI_API_KEY"):
        print("üîë Usando OpenAI Embeddings")
        return OpenAIEmbeddings()
    elif os.getenv("GOOGLE_API_KEY"):
        print("üîë Usando Google Gemini Embeddings")
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        return GoogleGenerativeAIEmbeddings()
    else:
        raise ValueError("Nenhuma API key encontrada no .env!")
```

**A√ß√£o**: Implementar e testar.

**Checkpoint**: Embeddings configurados corretamente.

### Passo 3.4: Implementar Armazenamento no PostgreSQL

**Arquivo**: `src/ingest.py` (continuar)

```python
from langchain_postgres import PGVector

def store_documents(chunks, embeddings, connection_string):
    """Armazena chunks e embeddings no PostgreSQL."""
    print("üíæ Armazenando no banco de dados...")
    try:
        vectorstore = PGVector(
            embeddings=embeddings,
            connection=connection_string,
            use_jsonb=True
        )
        
        # Adicionar documentos
        vectorstore.add_documents(chunks)
        print(f"‚úÖ {len(chunks)} documentos armazenados com sucesso!")
        return vectorstore
    except Exception as e:
        print(f"‚ùå Erro ao armazenar: {e}")
        raise
```

**A√ß√£o**: Implementar e testar.

**Checkpoint**: Documentos s√£o armazenados no banco.

### Passo 3.5: Implementar Fun√ß√£o Principal de Ingest√£o

**Arquivo**: `src/ingest.py` (completar)

```python
def ingest_pdf():
    """Fun√ß√£o principal para ingest√£o completa do PDF."""
    try:
        # 1. Carregar PDF
        documents = load_pdf(PDF_PATH)
        
        # 2. Dividir em chunks
        chunks = split_documents(documents)
        
        # 3. Configurar embeddings
        embeddings = get_embeddings()
        
        # 4. Obter string de conex√£o
        connection_string = os.getenv("POSTGRES_CONNECTION_STRING")
        if not connection_string:
            raise ValueError("POSTGRES_CONNECTION_STRING n√£o encontrada no .env!")
        
        # 5. Armazenar no banco
        vectorstore = store_documents(chunks, embeddings, connection_string)
        
        print("\nüéâ Ingest√£o conclu√≠da com sucesso!")
        return vectorstore
        
    except Exception as e:
        print(f"\n‚ùå Erro na ingest√£o: {e}")
        raise

if __name__ == "__main__":
    ingest_pdf()
```

**A√ß√£o**: Implementar fun√ß√£o completa e executar.

```bash
python src/ingest.py
```

**Checkpoint**: Ingest√£o completa funciona end-to-end.

### Passo 3.6: Verificar Dados no Banco

```bash
# Conectar ao banco
docker compose exec postgres psql -U postgres -d rag

# Verificar quantidade de chunks
SELECT COUNT(*) FROM langchain_pg_embedding;

# Ver alguns chunks
SELECT 
    id, 
    LEFT(document, 100) as preview,
    array_length(embedding::float[], 1) as embedding_dim
FROM langchain_pg_embedding 
LIMIT 5;

# Sair
\q
```

**Checkpoint**: Dados est√£o no banco, embeddings t√™m dimens√£o correta.

## üîç Fase 4: Implementa√ß√£o do M√≥dulo de Busca

### Passo 4.1: Implementar Fun√ß√£o de Busca Vetorial

**Arquivo**: `src/search.py` (atualizar)

```python
import os
from dotenv import load_dotenv
from langchain_postgres import PGVector
from langchain_openai import OpenAIEmbeddings
# OU from langchain_google_genai import GoogleGenerativeAIEmbeddings

load_dotenv()

PROMPT_TEMPLATE = """
CONTEXTO:
{context}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informa√ß√£o n√£o estiver explicitamente no CONTEXTO, responda:
  "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opini√µes ou interpreta√ß√µes al√©m do que est√° escrito.

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
Pergunta: "Qual √© a capital da Fran√ßa?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

Pergunta: "Quantos clientes temos em 2024?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

Pergunta: "Voc√™ acha isso bom ou ruim?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

PERGUNTA DO USU√ÅRIO:
{query}

RESPONDA A "PERGUNTA DO USU√ÅRIO"
"""

def get_vectorstore():
    """Retorna inst√¢ncia do vectorstore conectado ao banco."""
    connection_string = os.getenv("POSTGRES_CONNECTION_STRING")
    
    # Configurar embeddings (mesmo da ingest√£o)
    if os.getenv("OPENAI_API_KEY"):
        embeddings = OpenAIEmbeddings()
    elif os.getenv("GOOGLE_API_KEY"):
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        embeddings = GoogleGenerativeAIEmbeddings()
    else:
        raise ValueError("Nenhuma API key encontrada!")
    
    # Conectar ao vectorstore existente
    vectorstore = PGVector(
        embeddings=embeddings,
        connection=connection_string,
        use_jsonb=True
    )
    
    return vectorstore

def search_prompt(question: str):
    """Busca chunks relevantes e monta prompt com contexto."""
    if not question:
        return None
    
    try:
        # 1. Obter vectorstore
        vectorstore = get_vectorstore()
        
        # 2. Buscar k=10 chunks mais relevantes
        results = vectorstore.similarity_search_with_score(
            query=question,
            k=10
        )
        
        # 3. Montar contexto com os chunks
        context_parts = []
        for doc, score in results:
            context_parts.append(doc.page_content)
        
        context = "\n\n".join(context_parts)
        
        # 4. Montar prompt completo
        prompt = PROMPT_TEMPLATE.format(
            context=context,
            query=question
        )
        
        return prompt
        
    except Exception as e:
        print(f"‚ùå Erro na busca: {e}")
        return None
```

**A√ß√£o**: Implementar e testar fun√ß√£o.

```python
# Teste r√°pido
from search import search_prompt
prompt = search_prompt("Qual o faturamento?")
print(prompt[:500])  # Primeiros 500 caracteres
```

**Checkpoint**: Fun√ß√£o de busca retorna prompt formatado.

## üí¨ Fase 5: Implementa√ß√£o do M√≥dulo de Chat

### Passo 5.1: Configurar LLM

**Arquivo**: `src/chat.py` (atualizar)

```python
import os
from dotenv import load_dotenv
from search import search_prompt

load_dotenv()

def get_llm():
    """Retorna inst√¢ncia do LLM configurado."""
    if os.getenv("OPENAI_API_KEY"):
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.0,
            max_tokens=500
        )
    elif os.getenv("GOOGLE_API_KEY"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(
            model="gemini-pro",
            temperature=0.0,
            max_output_tokens=500
        )
    else:
        raise ValueError("Nenhuma API key encontrada!")
```

**Checkpoint**: LLM configurado corretamente.

### Passo 5.2: Implementar Loop de Chat

**Arquivo**: `src/chat.py` (continuar)

```python
def main():
    """Fun√ß√£o principal do chat CLI."""
    print("ü§ñ Chat de Busca Sem√¢ntica")
    print("=" * 50)
    print("Digite 'sair' para encerrar\n")
    
    try:
        # Inicializar LLM
        llm = get_llm()
        print("‚úÖ LLM inicializado com sucesso!\n")
    except Exception as e:
        print(f"‚ùå Erro ao inicializar LLM: {e}")
        return
    
    while True:
        try:
            # Receber pergunta do usu√°rio
            pergunta = input("Fa√ßa sua pergunta: ").strip()
            
            # Verificar se quer sair
            if pergunta.lower() in ['sair', 'exit', 'quit']:
                print("\nüëã Encerrando chat...")
                break
            
            # Verificar se pergunta n√£o est√° vazia
            if not pergunta:
                print("‚ö†Ô∏è  Por favor, digite uma pergunta v√°lida.\n")
                continue
            
            # Buscar contexto e montar prompt
            print("\nüîç Buscando informa√ß√µes...")
            prompt = search_prompt(pergunta)
            
            if not prompt:
                print("‚ùå Erro ao buscar informa√ß√µes. Tente novamente.\n")
                continue
            
            # Chamar LLM
            print("üí≠ Gerando resposta...\n")
            response = llm.invoke(prompt)
            
            # Extrair conte√∫do da resposta
            if hasattr(response, 'content'):
                resposta = response.content
            else:
                resposta = str(response)
            
            # Exibir resposta formatada
            print("PERGUNTA:", pergunta)
            print("RESPOSTA:", resposta)
            print("\n" + "-" * 50 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nüëã Encerrando chat...")
            break
        except Exception as e:
            print(f"\n‚ùå Erro: {e}\n")
            continue

if __name__ == "__main__":
    main()
```

**A√ß√£o**: Implementar fun√ß√£o completa.

**Checkpoint**: Chat CLI funciona corretamente.

### Passo 5.3: Melhorar Integra√ß√£o com LangChain

**Arquivo**: `src/search.py` (melhorar)

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

def create_rag_chain(llm):
    """Cria chain RAG completa."""
    prompt_template = PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["context", "query"]
    )
    
    return LLMChain(
        llm=llm,
        prompt=prompt_template
    )
```

**Arquivo**: `src/chat.py` (atualizar para usar chain)

```python
from search import get_vectorstore, create_rag_chain

def main():
    # ... c√≥digo anterior ...
    
    # Criar chain RAG
    chain = create_rag_chain(llm)
    vectorstore = get_vectorstore()
    
    # No loop:
    # Buscar contexto
    results = vectorstore.similarity_search_with_score(pergunta, k=10)
    context = "\n\n".join([doc.page_content for doc, _ in results])
    
    # Executar chain
    response = chain.invoke({"context": context, "query": pergunta})
    resposta = response["text"]
```

**Checkpoint**: Integra√ß√£o com LangChain completa.

## üß™ Fase 6: Testes e Valida√ß√£o

### Passo 6.1: Testar Ingest√£o Completa

```bash
# 1. Limpar dados anteriores (se necess√°rio)
docker compose down -v
docker compose up -d

# 2. Executar ingest√£o
python src/ingest.py

# 3. Verificar logs e output
# Deve mostrar: PDF carregado, chunks criados, documentos armazenados
```

**Checkpoint**: Ingest√£o completa sem erros.

### Passo 6.2: Testar Busca e Respostas

```bash
# Executar chat
python src/chat.py

# Testar perguntas:
# 1. Pergunta que deve estar no PDF
# 2. Pergunta que n√£o deve estar no PDF
# 3. Pergunta que requer opini√£o
# 4. Pergunta sobre conhecimento geral
```

**Checkpoint**: Todas as respostas s√£o apropriadas.

### Passo 6.3: Validar Comportamento Esperado

**Cen√°rios de Teste:**

1. **Pergunta com resposta no contexto**
   - Input: "Qual o faturamento da empresa?"
   - Output esperado: Resposta baseada no PDF

2. **Pergunta sem resposta no contexto**
   - Input: "Quantos clientes temos em 2024?"
   - Output esperado: "N√£o tenho informa√ß√µes necess√°rias..."

3. **Pergunta que requer opini√£o**
   - Input: "Voc√™ acha isso bom ou ruim?"
   - Output esperado: "N√£o tenho informa√ß√µes necess√°rias..."

4. **Pergunta sobre conhecimento geral**
   - Input: "Qual √© a capital da Fran√ßa?"
   - Output esperado: "N√£o tenho informa√ß√µes necess√°rias..."

**Checkpoint**: Todos os cen√°rios funcionam corretamente.

## üîß Fase 7: Melhorias e Ajustes

### Passo 7.1: Adicionar Tratamento de Erros Robusto

- Validar entrada do usu√°rio
- Tratar erros de conex√£o
- Tratar erros de API
- Mensagens de erro amig√°veis

### Passo 7.2: Adicionar Logging

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)
```

### Passo 7.3: Melhorar UX do CLI

- Indicadores de progresso
- Formata√ß√£o melhorada
- Cores (opcional com `colorama`)
- Hist√≥rico de perguntas (opcional)

### Passo 7.4: Valida√ß√£o de Dados

- Verificar se PDF existe antes de processar
- Verificar se banco est√° acess√≠vel
- Verificar API keys antes de usar
- Validar formato do PDF

## üìä Checklist Final de Implementa√ß√£o

### Configura√ß√£o
- [ ] Ambiente virtual criado
- [ ] Depend√™ncias instaladas
- [ ] Arquivo `.env` configurado
- [ ] PostgreSQL rodando
- [ ] pgVector instalado

### Ingest√£o (`src/ingest.py`)
- [ ] Carregamento de PDF implementado
- [ ] Divis√£o em chunks (1000 chars, 150 overlap)
- [ ] Gera√ß√£o de embeddings
- [ ] Armazenamento no PostgreSQL
- [ ] Fun√ß√£o `ingest_pdf()` completa

### Busca (`src/search.py`)
- [ ] Template de prompt definido
- [ ] Fun√ß√£o `get_vectorstore()` implementada
- [ ] Fun√ß√£o `search_prompt()` implementada
- [ ] Busca vetorial (k=10) funcionando

### Chat (`src/chat.py`)
- [ ] LLM configurado
- [ ] Loop de chat implementado
- [ ] Integra√ß√£o com busca
- [ ] Formata√ß√£o de resposta
- [ ] Tratamento de sa√≠da

### Valida√ß√£o
- [ ] Ingest√£o completa funciona
- [ ] Chat funciona corretamente
- [ ] Respostas baseadas no contexto
- [ ] Respostas para perguntas fora do contexto
- [ ] Tratamento de erros

## üöÄ Ordem de Execu√ß√£o Final

```bash
# 1. Configurar ambiente
source venv/bin/activate
pip install -r requirements.txt

# 2. Configurar vari√°veis
cp .env.example .env
# Editar .env

# 3. Iniciar banco
docker compose up -d

# 4. Ingerir PDF
python src/ingest.py

# 5. Executar chat
python src/chat.py
```

## üìù Notas Importantes

1. **API Keys**: Nunca commitar arquivo `.env` no Git
2. **Custos**: Cada ingest√£o e pergunta gera custos na API
3. **Performance**: Ingest√£o pode ser lenta para PDFs grandes
4. **Embeddings**: Use o mesmo modelo de embeddings na ingest√£o e busca
5. **Banco de Dados**: Dados persistem no volume Docker

## üêõ Troubleshooting R√°pido

| Problema               | Solu√ß√£o                                      |
| ---------------------- | -------------------------------------------- |
| PDF n√£o encontrado     | Verificar `PDF_PATH` no `.env`               |
| Erro de conex√£o        | Verificar `docker compose ps`                |
| API Key inv√°lida       | Verificar `.env` e cr√©ditos na conta         |
| Embeddings n√£o gerados | Verificar API key e conex√£o                  |
| Respostas incorretas   | Verificar se ingest√£o foi feita corretamente |

## üìö Refer√™ncias

- Arquitetura: `docs/architecture.md`
- Estrat√©gia de Dados: `docs/data-strategy.md`
- Prompt Engineering: `docs/prompt-engineering.md`
- Deployment: `docs/deployment.md`
- Requisitos: `docs/requirements.md`

---

**Status**: üìã Plano de a√ß√£o pronto para implementa√ß√£o

**Pr√≥ximo passo**: Come√ßar pela Fase 1 e seguir sequencialmente cada passo.

