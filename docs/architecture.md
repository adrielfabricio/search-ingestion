# Arquitetura do Sistema - Ingest√£o e Busca Sem√¢ntica

## Vis√£o Geral

Sistema de busca sem√¢ntica (RAG - Retrieval Augmented Generation) que permite consultar documentos PDF atrav√©s de perguntas em linguagem natural, utilizando LangChain, PostgreSQL com pgVector e modelos de IA.

## Arquitetura de Alto N√≠vel

```mermaid
flowchart TB
    subgraph Ingestao["üì• FASE DE INGEST√ÉO (ingest.py)"]
        A[PDF File] --> B[PyPDFLoader<br/>Carrega PDF]
        B --> C[RecursiveCharacterSplitter<br/>Chunks: 1000 chars<br/>Overlap: 150 chars]
        C --> D[OpenAIEmbeddings<br/>Gera vetores]
        D --> E[PGVector<br/>Armazena no PostgreSQL]
    end
    
    E --> F[(PostgreSQL + pgVector<br/>Armazena chunks e embeddings)]
    
    subgraph Busca["üîç FASE DE BUSCA RAG (chat.py)"]
        G[Usu√°rio faz pergunta] --> H[Vetoriza pergunta<br/>OpenAIEmbeddings]
        H --> I[Busca vetorial<br/>similarity_search<br/>k=10 chunks mais relevantes]
        I --> J[Monta prompt com contexto<br/>search.py]
        J --> K[LLM gera resposta<br/>OpenAI/Gemini]
        K --> L[Retorna resposta ao usu√°rio<br/>via CLI]
    end
    
    F --> H
    
    style A fill:#e1f5ff
    style Ingestao fill:#fff4e1,stroke:#ff9800,stroke-width:2px
    style F fill:#e8f5e9,stroke:#4caf50,stroke-width:3px
    style Busca fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    style L fill:#e8f5e9
```

## Componentes Principais

### 1. M√≥dulo de Ingest√£o (`src/ingest.py`)

**Responsabilidades:**
- Carregar arquivo PDF usando `PyPDFLoader`
- Dividir texto em chunks de 1000 caracteres com overlap de 150
- Gerar embeddings para cada chunk
- Armazenar chunks e embeddings no PostgreSQL com pgVector

**Fluxo:**
1. Carrega PDF ‚Üí Extrai texto
2. Divide em chunks ‚Üí Lista de documentos
3. Gera embeddings ‚Üí Vetores num√©ricos
4. Armazena no banco ‚Üí Chunks + embeddings

### 2. M√≥dulo de Busca (`src/search.py`)

**Responsabilidades:**
- Definir template de prompt para RAG
- Realizar busca vetorial por similaridade
- Montar contexto para o LLM

**Fluxo:**
1. Recebe pergunta do usu√°rio
2. Vetoriza pergunta
3. Busca k=10 chunks mais similares
4. Monta prompt com contexto
5. Retorna prompt pronto para LLM

### 3. M√≥dulo de Chat (`src/chat.py`)

**Responsabilidades:**
- Interface CLI para intera√ß√£o com usu√°rio
- Orquestrar busca e gera√ß√£o de resposta
- Exibir resposta formatada

**Fluxo:**
1. Loop de intera√ß√£o com usu√°rio
2. Chama `search_prompt()` para buscar contexto
3. Envia prompt para LLM
4. Exibe resposta formatada

### 4. Banco de Dados (PostgreSQL + pgVector)

**Estrutura:**
- Tabela criada automaticamente pelo LangChain
- Armazena:
  - Texto original do chunk
  - Embedding vetorial
  - Metadados (opcional)

**Extens√£o pgVector:**
- Permite busca por similaridade vetorial
- Usa dist√¢ncia cosseno para compara√ß√£o
- √çndices otimizados para busca r√°pida

## Fluxo de Dados

### Fase de Ingest√£o

```
PDF ‚Üí Texto ‚Üí Chunks ‚Üí Embeddings ‚Üí PostgreSQL
```

### Fase de Consulta (RAG)

```
Pergunta ‚Üí Embedding ‚Üí Busca Vetorial ‚Üí Chunks Relevantes ‚Üí 
Prompt + Contexto ‚Üí LLM ‚Üí Resposta
```

## Tecnologias e Depend√™ncias

### Stack Principal
- **Python 3.9+**: Linguagem de programa√ß√£o
- **LangChain**: Framework para aplica√ß√µes LLM
- **PostgreSQL 17**: Banco de dados relacional
- **pgVector**: Extens√£o para busca vetorial
- **Docker**: Containeriza√ß√£o do banco de dados

### Bibliotecas Espec√≠ficas
- `langchain_community.document_loaders.PyPDFLoader`: Carregamento de PDF
- `langchain_text_splitters.RecursiveCharacterTextSplitter`: Divis√£o de texto
- `langchain_openai.OpenAIEmbeddings`: Gera√ß√£o de embeddings (OpenAI)
- `langchain_google_genai.GoogleGenerativeAIEmbeddings`: Gera√ß√£o de embeddings (Gemini)
- `langchain_postgres.PGVector`: Integra√ß√£o com PostgreSQL vetorial
- `langchain_openai.ChatOpenAI`: Modelo LLM (OpenAI)
- `langchain_google_genai.ChatGoogleGenerativeAI`: Modelo LLM (Gemini)

## Decis√µes Arquiteturais

### 1. Chunking Strategy
- **Tamanho**: 1000 caracteres
- **Overlap**: 150 caracteres
- **Justificativa**: 
  - 1000 chars √© um bom equil√≠brio entre contexto e precis√£o
  - Overlap garante que informa√ß√µes n√£o sejam perdidas nas bordas

### 2. N√∫mero de Chunks (k=10)
- **Justificativa**: 
  - 10 chunks fornecem contexto suficiente
  - N√£o sobrecarrega o prompt do LLM
  - Balance entre precis√£o e custo

### 3. PostgreSQL + pgVector
- **Justificativa**:
  - Solu√ß√£o open-source robusta
  - Integra√ß√£o nativa com LangChain
  - Suporte a busca vetorial eficiente
  - Persist√™ncia de dados

### 4. CLI em vez de API
- **Justificativa**:
  - Simplicidade para MVP
  - Foco em funcionalidade core
  - Facilita testes e desenvolvimento

## Considera√ß√µes de Performance

- **Ingest√£o**: Processamento √∫nico, pode ser lento
- **Busca**: Otimizada com √≠ndices vetoriais do pgVector
- **LLM**: Lat√™ncia depende do provedor (OpenAI/Gemini)
- **Custo**: Principalmente na gera√ß√£o de embeddings e chamadas LLM

## Seguran√ßa

- **API Keys**: Armazenadas em vari√°veis de ambiente (.env)
- **Dados**: PDFs e embeddings armazenados localmente
- **Banco de Dados**: Configurado para desenvolvimento (n√£o produ√ß√£o)

## Limita√ß√µes Conhecidas

- Busca limitada ao conte√∫do do PDF ingerido
- N√£o h√° persist√™ncia de hist√≥rico de conversas
- CLI simples, sem recursos avan√ßados de UI
- Processamento de PDFs grandes pode ser lento

